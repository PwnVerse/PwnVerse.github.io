<!doctype html><html class="dark light"><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         Mitigations of the Linux Kernel
        
    </title><meta content="Mitigations of the Linux Kernel" property=og:title><link href=https://pwnverse.github.io/fonts.css rel=stylesheet><script>(function(){var a=document.createElement('script');a.setAttribute('src','/js/imamu.js');a.setAttribute('data-website-id','74344cd8-1e87-4c38-8acd-f96049b8f07e');a.setAttribute('data-host-url','https:&#x2F;&#x2F;analytics.eu.umami.is');document.body.appendChild(a)})()</script><script async data-host-url=https://analytics.eu.umami.is data-website-id=74344cd8-1e87-4c38-8acd-f96049b8f07e src=/js/imamu.js></script><script async data-goatcounter=https://Cyb0rG.goatcounter.com/count src=https://pwnverse.github.io/js/count.js></script><noscript><img src="https://Cyb0rG.goatcounter.com//count?p=/posts/mitigations/&t=Mitigations of the Linux Kernel"></noscript><link title="Cyb0rG's Home" href=https://pwnverse.github.io/atom.xml rel=alternate type=application/atom+xml><link href=https://pwnverse.github.io/theme/light.css rel=stylesheet><link href=https://pwnverse.github.io/theme/dark.css id=darkModeStyle rel=stylesheet><link href=https://pwnverse.github.io/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><a href=https://pwnverse.github.io>Cyb0rG's Home</a><div class=socials><a class=social href=https://twitter.com/_Cyb0rG rel=me> <img alt=twitter src=/social_icons/twitter.svg> </a><a class=social href=https://github.com/PwnVerse/ rel=me> <img alt=github src=/social_icons/github.svg> </a></div></div><nav><a href=/posts style=margin-left:.7em>/posts</a><a href=/projects style=margin-left:.7em>/projects</a><a href=/about style=margin-left:.7em>/about</a> | <a href=javascript:void(0) id=dark-mode-toggle onclick=toggleTheme()> <img id=sun-icon src=/feather/sun.svg style=filter:invert(1)> <img id=moon-icon src=/feather/moon.svg> </a><script src=https://pwnverse.github.io/js/themetoggle.js></script></nav></header><main><article><div class=title><div class=page-header>Mitigations of the Linux Kernel<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2021-02-26</time></div></div><section class=body><p>I've been looking into some linux kernel lately and this blog post is to get familiar with all kernel mitigations like <code>SMEP</code> , <code>SMAP</code> , <code>KPTI</code> and <code>FG-KASLR</code>. This post is a beginner friendly one (as I myself am a noob).<h1 id=the-story-of-kernel-mitigations>The story of kernel mitigations</h1><p>The linux kernel has come a long way from being a primitive micro kernel to a super powerful (and relatively secure) micro kernel. Back in the early 90's there were almost no protections in the baby linux kernel. If an attacker gets code execution in the kernel , the attacker becomes capable of running code in the very ring zero ie, with highest possible privileges (root). Since the linux kernel is a large piece of software, so it has a lot of attack vectors/structures which can be used to immediately escalate our privileges to root (another post on attack vectors in kernel soon).<p>One thing that makes kernel exploitation so interesting is that you control the entire userspace operations , you have to make any possible function calls, leak kernel pointers, overwrite useful structures in the kernel and get root.<h2 id=mitigation-1-ret2user>Mitigation #1 - Ret2user</h2><p>For those familiar with the technique of <code>ret2shellcode</code> in userspace exploitation , this is almost the same stuff. If you have a null pointer dereference in older kernels , you can easily get root as the saying goes "One null deref can root them all".<p>So, a mitigation against this technique was <code>KERNEXEC</code> which basically sets syscall table, Interrupt Descriptor Table, Global Descriptor Table, some page tables to <code>RO</code> and set <code>data pages</code> to NX. As we can see, making kernel pages RO is super helpful to minimizing the attack surfaces and <a href=https://cs.brown.edu/~vpk/papers/ret2dir.sec14.pdf>ret2dir</a> attacks.<p>Following <code>KERNEXEC</code>, the well known <code>SMEP</code> mitigation had rolled out to provide a subset of functions of <code>KERNEXEC</code>. <code>SMEP</code> prevents stuff like mmaping and executing shellcode to get root. It basically prevents execution of any page which is not a part of the kernel space. But unlike <code>KERNEXEC</code>, it doesnt prevent exploitation of <code>RWX</code> or important kernel data structure. In kernel, this is enabled by setting the <code>20th bit</code> of the <code>CR4</code> control register.<h2 id=bypassing-smep>Bypassing SMEP</h2><p>With some ROP , we can unset the 20th of the CR4 register, and to our delight, there's a function in the kernel which does exactly the same thing, <code>mov cr4,edi</code>and the function is <code>native_write_cr4</code>.<p>But wait there's a cakehole we're going towards. In reality, the latest kernels have this super cool feature which <code>pins</code> or <code>hardcodes</code> the <code>CR4</code> register thus attempting to change it is practically impossible. Hence, we'll have to ROP our way to previlege escalation with <code>commit_creds(prepare_kernel_cred(0))</code>.<p>Here's the relevant code to end our ecstasy.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>void </span><span style=color:#ffb454>native_write_cr4</span><span>(</span><span style=color:#ff7733>unsigned long </span><span style=color:#f29718>val</span><span>)
</span><span>{
</span><span>	</span><span style=color:#ff7733>unsigned long</span><span> bits_changed </span><span style=color:#f29668>= </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>;
</span><span>
</span><span style=color:#59c2ff>set_register</span><span style=color:#bfbab0cc>:
</span><span>	</span><span style=color:#ff7733>asm volatile</span><span>(</span><span style=color:#c2d94c>"mov </span><span style=color:#f29718>%0,%%c</span><span style=color:#c2d94c>r4"</span><span style=color:#f29668>: </span><span style=color:#c2d94c>"+r" </span><span>(val) </span><span style=color:#f29668>: : </span><span style=color:#c2d94c>"memory"</span><span>)</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>static_branch_likely</span><span>(</span><span style=color:#f29668>&</span><span>cr_pinning)) {
</span><span>		</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>unlikely</span><span>((val </span><span style=color:#f29668>&</span><span> cr4_pinned_mask) </span><span style=color:#f29668>!=</span><span> cr4_pinned_bits)) {
</span><span>			bits_changed </span><span style=color:#f29668>= </span><span>(val </span><span style=color:#f29668>&</span><span> cr4_pinned_mask) </span><span style=color:#f29668>^</span><span> cr4_pinned_bits</span><span style=color:#bfbab0cc>;
</span><span>			val </span><span style=color:#f29668>= </span><span>(val </span><span style=color:#f29668>& ~</span><span>cr4_pinned_mask) </span><span style=color:#f29668>|</span><span> cr4_pinned_bits</span><span style=color:#bfbab0cc>;
</span><span>			</span><span style=color:#ff7733>goto</span><span> set_register</span><span style=color:#bfbab0cc>;
</span><span>		}
</span><span>		</span><span style=font-style:italic;color:#5c6773>/* Warn after we've corrected the changed bits. */
</span><span>		</span><span style=color:#ffb454>WARN_ONCE</span><span>(bits_changed</span><span style=color:#bfbab0cc>, </span><span style=color:#c2d94c>"pinned CR4 bits changed: 0x</span><span style=color:#f29718>%lx</span><span style=color:#c2d94c>!?</span><span style=color:#95e6cb>\n</span><span style=color:#c2d94c>"</span><span style=color:#bfbab0cc>,
</span><span>			  bits_changed)</span><span style=color:#bfbab0cc>;
</span><span>	}
</span><span>}
</span></code></pre><p>Hence, to bypass <code>SMEP</code>, we can do some ROP to set <code>commit_creds(prepare_kernel_creds(0))</code>.<h2 id=mitigation-2-accessing-user-memory-when-executing-kernel-code>Mitigation #2 - Accessing User memory when executing kernel code</h2><p>Any attempt by user space program to examine or modify the kernel's part of the address space will result in a plain segfault as we've seen until now. But the access in the other direction (kernel modifying userspace memory) is much less controlled. when the processor is in kernel mode, it has full access to any address that is valid in the page tables. Or nearly full access; the processor will still not normally allow writes to read-only memory, but that check can be disabled when the need arises.<p>Intel's <code>Supervisor Mode Access Prevention</code> (SMAP) which came in around 2012 with the linux kernel 3.7 completely changed this situation. This extension defines a new SMAP bit in the <code>CR4 control register</code> (21st bit) which when set, any attempt to access user-space memory while running in a privileged mode will lead to a page fault.<p>Naturally, there are times when the kernel needs to work with user-space memory. To that end, Intel has defined a separate <code>AC</code> flag that controls the SMAP feature. If the AC flag is set, SMAP protection is in force, otherwise access to user-space memory is allowed. To achieve this , two new instructions (<code>STAC</code> and <code>CLAC</code>) were introduced to modify that flag quickly. User-space access functions (<code>get_user()</code>, for example, or <code>copy_to_user()</code>) clearly need to have user-space access enabled.<p>Well, now you may ask, why do we even need this mitigation if the kernel itself can modify it's own access to the userspace memory. The answer is it can block a whole class of exploits where the kernel is fooled into reading from (or <strong>writing</strong> to) user-space memory by mistake.<h2 id=bypassing-smap>Bypassing SMAP</h2><p>The <code>SMAP</code> can be bypassed with kernel based <code>Return oriented Programming</code> only in the kernel space to get root.<h2 id=mitigation-3-address-space-randomization-in-the-kernel>Mitigation #3 - Address Space Randomization in the kernel</h2><p>Not very long from now, in 2014, the <code>KASLR</code> was finally merged into the mainstream vanilla linux kernel 3.14. As one can expects, it randomizes entire address space from the base address. But as you can probably think now, defeating KASLR is pretty trivial since virtually every kernel address is offsetable from the base address. So, this technique wasn't really a hit in the community.<h2 id=mitigation-4-page-table-isolation>Mitigation #4 - Page Table Isolation</h2><p>Page Table Isolation (pti, previously known as KAISER) is a countermeasure against attacks on the shared user/kernel address space such as the <a href=https://meltdownattack.com/>Meltdown</a>. This mitigation came into the linux kernel 4.15. <code>Page-table</code> entries contain permission bits describing how the memory they describe can be accessed; these bits are, naturally, set to prevent user space from accessing kernel pages, even though those pages are mapped into the address space.<p>But, a number of hardware level bugs allow a user-space hacker to determine whether a given kernel-space address is mapped or not, regardless of whether any page mapped at that address is accessible. This basic information can be used to defeat KASLR.<p>On a system with 4 levels of page tables ,the top level is the <code>Page Global Directory</code> (PGD), below that come the <code>Page upper directory</code> (PUD), <code>Page Middle Directory</code> (PMD) and <code>Page Table Entries</code> (PTE). Page-table resolution normally traverses the entire page table tree to find the <code>PTE</code> of interest. The <code>PTE</code> is actually used to translate <code>virtual memory address</code> (CPU generated) to <code>physical memory address</code>(actual location on RAM).<p>One of the first steps taken in the <code>KPTI</code> patch is to create a second <code>Page Global Directory</code> one for kernel space and the other one when the program is running in userspace. So , in whole, the KPTI separates kernel and user page tables.<h2 id=bypassing-kpti>Bypassing KPTI</h2><ul><li><p>Using a <code>KPTI trampoline</code> this method is based on the idea that if a syscall returns normally, there must be a piece of code in the kernel that will swap the page tables back to the userland ones, so we will try to reuse that code to our purpose. The function is <code>swapgs_restore_regs_and_return_to_usermode</code>, and what it does is to swap page tables, <code>swapgs</code> and <code>iretq</code>.</p><li><p>Another weird technique is leveraging the power of <code>signal handlers</code>. So, when we try to return to usermode and execute code, the page table still being used is the <code>kernel page table</code> which is by default prevented to execute code in the usermode and hence a <code>segfault</code> happens in the <strong>userspace</strong>. If a segfault handler is used instead, and to handle the <code>SIGSEGV</code> signal, if we call our <code>shell</code> function, we get root shell given that we have already set the <code>commit_creds(prepare_kernel_creds(0))</code>.</p></ul><h2 id=mitigation-5-function-granular-kernel-address-space-randomization>Mitigation #5 - Function Granular Kernel Address Space Randomization</h2><p>This is the latest mitigation of the linux kernel version 5.11. With FGKASLR, individual kernel functions are reordered so that even if the kernel's randomized based address is revealed, an attacker still wouldn't know the location in memory of particular kernel functions as the relative addresses will be different. <code>FGKASLR</code> reorders the functions at boot time and is a further improvement to Linux security for attacks that require known positions within the kernel memory.<h2 id=bypassing-fg-kaslr>Bypassing FG-KASLR</h2><p>The only way to bypass FGKASLR is to search for constant offset functions by leaking multiple pointers in various parts of the desired kernel memory. There are a few functions which are not effected by the FGKASLR at all, so we hunt for such functions.<h1 id=conclusion>Conclusion</h1><p>Linux has come a long way now with all these mitigations. Yet, vulnerabilites and bypasses never stop coming into the way of security and hence newer mitigations are only a few lines of code away.<h3 id=references>References</h3><ul><li><a href="https://www.phoronix.com/scan.php?page=news_item&px=Intel-Linux-FGKASLR-Proposal">FGKASLR</a><li><a href=https://trungnguyen1909.github.io/blog/post/matesctf/KSMASH/>KPTI Trampoline</a><li><a href=https://lwn.net/Articles/741878/>KPTI</a><li><a href=https://lwn.net/Articles/517475/>SMAP</a><li><a href=https://github.com/pr0cf5/kernel-exploit-practice/tree/master/bypass-smep>SMEP</a><li><a href=https://lkmidas.github.io/posts/20210128-linux-kernel-pwn-part-2/>An Awesome blog post</a></ul></section></article></main></div>