<!doctype html><html class="dark light"><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         Allocators of the Linux Kernel
        
    </title><meta content="Allocators of the Linux Kernel" property=og:title><link href=https://pwnverse.github.io/fonts.css rel=stylesheet><script>(function(){var a=document.createElement('script');a.setAttribute('src','/js/imamu.js');a.setAttribute('data-website-id','74344cd8-1e87-4c38-8acd-f96049b8f07e');a.setAttribute('data-host-url','https:&#x2F;&#x2F;analytics.eu.umami.is');document.body.appendChild(a)})()</script><script async data-host-url=https://analytics.eu.umami.is data-website-id=74344cd8-1e87-4c38-8acd-f96049b8f07e src=/js/imamu.js></script><script async data-goatcounter=https://Cyb0rG.goatcounter.com/count src=https://pwnverse.github.io/js/count.js></script><noscript><img src="https://Cyb0rG.goatcounter.com//count?p=/posts/slaballocator/&t=Allocators of the Linux Kernel"></noscript><link title="Cyb0rG's Home" href=https://pwnverse.github.io/atom.xml rel=alternate type=application/atom+xml><link href=https://pwnverse.github.io/theme/light.css rel=stylesheet><link href=https://pwnverse.github.io/theme/dark.css id=darkModeStyle rel=stylesheet><link href=https://pwnverse.github.io/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><a href=https://pwnverse.github.io>Cyb0rG's Home</a><div class=socials><a class=social href=https://twitter.com/_Cyb0rG rel=me> <img alt=twitter src=/social_icons/twitter.svg> </a><a class=social href=https://github.com/PwnVerse/ rel=me> <img alt=github src=/social_icons/github.svg> </a></div></div><nav><a href=/posts style=margin-left:.7em>/posts</a><a href=/projects style=margin-left:.7em>/projects</a><a href=/about style=margin-left:.7em>/about</a> | <a href=javascript:void(0) id=dark-mode-toggle onclick=toggleTheme()> <img id=sun-icon src=/feather/sun.svg style=filter:invert(1)> <img id=moon-icon src=/feather/moon.svg> </a><script src=https://pwnverse.github.io/js/themetoggle.js></script></nav></header><main><article><div class=title><div class=page-header>Allocators of the Linux Kernel<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2021-01-22</time></div></div><section class=body><h2 id=tl-dr>tl;dr</h2><ul><li>Get an insight into the workings of the different alloctors implemented in the linux kernel.<li>Venture a little into the source code.</ul><h2 id=the-slob-allocator>The Slob Allocator</h2><p>Simple List of Blocks allocator is one of the three main allocators in the linux kernel. It is mostly used in embedded systems where memory is expensive. It works using the <strong>first fit</strong> algorithm and hence leads to fragmentation.<h2 id=the-slab-allocator>The Slab Allocator</h2><p>It fixes the shortcomings of the slob allocator<p>The basic idea behind the slab allocator is to have caches of commonly used objects kept in an initialized state ready for use by the kernel. So why this object based allocation one may ask?<p>Without an object based allocator , the kernel will spend more time in allocation , initializing and freeing the same object. {: .notice}<p>The slab allocator consists of a variable number of caches that are linked together on a circular doubly linked list called <strong>cache chain</strong>. Each cache maintains blocks of contiguous pages in memory called <strong>slabs</strong>.<p><img alt src=https://ibb.co/TY8m0Fz><h2 id=principles-of-slab-allocator>Principles of Slab allocator</h2><ul><li>The allocation of small blocks of memory to help eliminate internal fragmentation.<li>Caching of commonly used objects so that the kernel doesnt waste time.<li>Better utilization of hardware cache by aligning objects to the L1 or L2 caches.</ul><p>In a general sense , the slab allocator in sits on top of a bump allocator called <strong>buddy allocator</strong> and makes sure that commonly used objects are cached.<p>The slab allocator provides Dedicated and generic cache classes. The separation between cache classes can be seen in a proc file called <strong>/proc/slabinfo</strong>.<h3 id=interfaces-of-kernel-memory-allocation>Interfaces of kernel memory allocation</h3><p><strong>kmalloc</strong> is the general interface the kernel provides to do generic allocations through slab allocator. There are also others like <strong>kzalloc</strong> (similar to glibc calloc) and krealloc.<h2 id=how-does-a-heap-chunk-look-like-in-the-kernel>How does a heap chunk look like in the kernel?</h2><p>With the slab allocator , kmalloc'd chunks <strong>dont have any metadata</strong> like headers for size in glibc. The free chunks obviously are stored in a single linked list.<ul><li>A slab allocator interface like kmalloc searches the right <strong>kmem_cache structure</strong> that serves a given size of allocation.</ul><h2 id=a-little-exploration-of-the-source-code>A little exploration of the source code</h2><p>For sizes less than or equal to <strong>192</strong>, kmalloc maintains immediate caches of sizes of range 8 to 192.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>static</span><span> __always_inline </span><span style=color:#ff7733>void </span><span style=color:#f29668>*</span><span style=color:#ffb454>__do_kmalloc</span><span>(</span><span style=font-style:italic;color:#39bae6>size_t </span><span style=color:#f29718>size</span><span style=color:#bfbab0cc>,</span><span> gfp_t </span><span style=color:#f29718>flags</span><span style=color:#bfbab0cc>,
</span><span>					  </span><span style=color:#ff7733>unsigned long </span><span style=color:#f29718>caller</span><span>)
</span><span>{
</span><span>	</span><span style=color:#ff7733>struct</span><span> kmem_cache </span><span style=color:#f29668>*</span><span>cachep</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ff7733>void </span><span style=color:#f29668>*</span><span>ret</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>unlikely</span><span>(size </span><span style=color:#f29668>></span><span> KMALLOC_MAX_CACHE_SIZE))
</span><span>		</span><span style=color:#ff7733>return </span><span style=color:#f29718>NULL</span><span style=color:#bfbab0cc>;
</span><span>	cachep </span><span style=color:#f29668>= </span><span style=color:#ffb454>kmalloc_slab</span><span>(size</span><span style=color:#bfbab0cc>,</span><span> flags)</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>unlikely</span><span>(</span><span style=color:#ffb454>ZERO_OR_NULL_PTR</span><span>(cachep)))
</span><span>		</span><span style=color:#ff7733>return</span><span> cachep</span><span style=color:#bfbab0cc>;
</span><span>	ret </span><span style=color:#f29668>= </span><span style=color:#ffb454>slab_alloc</span><span>(cachep</span><span style=color:#bfbab0cc>,</span><span> flags</span><span style=color:#bfbab0cc>,</span><span> caller)</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	ret </span><span style=color:#f29668>= </span><span style=color:#ffb454>kasan_kmalloc</span><span>(cachep</span><span style=color:#bfbab0cc>,</span><span> ret</span><span style=color:#bfbab0cc>,</span><span> size</span><span style=color:#bfbab0cc>,</span><span> flags)</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ffb454>trace_kmalloc</span><span>(caller</span><span style=color:#bfbab0cc>,</span><span> ret</span><span style=color:#bfbab0cc>,
</span><span>		      size</span><span style=color:#bfbab0cc>,</span><span> cachep</span><span style=color:#f29668>-></span><span>size</span><span style=color:#bfbab0cc>,</span><span> flags)</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	</span><span style=color:#ff7733>return</span><span> ret</span><span style=color:#bfbab0cc>;
</span><span>}
</span></code></pre><p>The call to kmalloc_slab is demonstrated.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>struct</span><span> kmem_cache </span><span style=color:#f29668>*</span><span style=color:#ffb454>kmalloc_slab</span><span>(</span><span style=font-style:italic;color:#39bae6>size_t </span><span style=color:#f29718>size</span><span style=color:#bfbab0cc>,</span><span> gfp_t </span><span style=color:#f29718>flags</span><span>)
</span><span>{
</span><span>	</span><span style=color:#ff7733>unsigned int</span><span> index</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	</span><span style=color:#ff7733>if </span><span>(size </span><span style=color:#f29668><= </span><span style=color:#f29718>192</span><span>) {
</span><span>		</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#f29668>!</span><span>size)
</span><span>			</span><span style=color:#ff7733>return</span><span> ZERO_SIZE_PTR</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>		index </span><span style=color:#f29668>=</span><span> size_index[</span><span style=color:#ffb454>size_index_elem</span><span>(size)]</span><span style=color:#bfbab0cc>;
</span><span>	} </span><span style=color:#ff7733>else </span><span>{
</span><span>		</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>WARN_ON_ONCE</span><span>(size </span><span style=color:#f29668>></span><span> KMALLOC_MAX_CACHE_SIZE))
</span><span>			</span><span style=color:#ff7733>return </span><span style=color:#f29718>NULL</span><span style=color:#bfbab0cc>;
</span><span>		index </span><span style=color:#f29668>= </span><span style=color:#ffb454>fls</span><span>(size </span><span style=color:#f29668>- </span><span style=color:#f29718>1</span><span>)</span><span style=color:#bfbab0cc>;
</span><span>	}
</span><span>
</span><span>	</span><span style=color:#ff7733>return</span><span> kmalloc_caches[</span><span style=color:#ffb454>kmalloc_type</span><span>(flags)][index]</span><span style=color:#bfbab0cc>;
</span><span>}
</span></code></pre><ul><li>If requested size is greater than <strong>KMALLOC_MAX_CACHE_SIZE</strong> which is the size of two pages <strong>8192</strong> , then a call to <strong>kmalloc_large</strong> is made.<li>The call to <strong>kmalloc_slab</strong> gets a cache pool using size based indexing.</ul><p>The <strong>slub_alloc</strong> calls another function <strong>slab_alloc_node</strong>.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>static</span><span> __always_inline </span><span style=color:#ff7733>void </span><span style=color:#f29668>*</span><span style=color:#ffb454>slab_alloc</span><span>(</span><span style=color:#ff7733>struct</span><span> kmem_cache </span><span style=color:#f29668>*</span><span style=color:#f29718>s</span><span style=color:#bfbab0cc>,
</span><span>		gfp_t </span><span style=color:#f29718>gfpflags</span><span style=color:#bfbab0cc>, </span><span style=color:#ff7733>unsigned long </span><span style=color:#f29718>addr</span><span>)
</span><span>{
</span><span>	</span><span style=color:#ff7733>return </span><span style=color:#ffb454>slab_alloc_node</span><span>(s</span><span style=color:#bfbab0cc>,</span><span> gfpflags</span><span style=color:#bfbab0cc>,</span><span> NUMA_NO_NODE</span><span style=color:#bfbab0cc>,</span><span> addr)</span><span style=color:#bfbab0cc>;
</span><span>}
</span><span>
</span></code></pre><p>Here, the variable <strong>NUMA_NO_NODE</strong> that is <strong>Non uniform memory access</strong> cells.<p>From a hardware aspect, a NUMA system can be assumed to be cells of memory each of which is in itself a symmetric memory processor subset of the system. They provide a scalable memory bandwidth. {: .notice}<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>static</span><span> __always_inline </span><span style=color:#ff7733>void </span><span style=color:#f29668>*</span><span style=color:#ffb454>slab_alloc_node</span><span>(</span><span style=color:#ff7733>struct</span><span> kmem_cache </span><span style=color:#f29668>*</span><span style=color:#f29718>s</span><span style=color:#bfbab0cc>,
</span><span>		gfp_t </span><span style=color:#f29718>gfpflags</span><span style=color:#bfbab0cc>, </span><span style=color:#ff7733>int </span><span style=color:#f29718>node</span><span style=color:#bfbab0cc>, </span><span style=color:#ff7733>unsigned long </span><span style=color:#f29718>addr</span><span>)
</span><span>{
</span><span>	</span><span style=color:#ff7733>void </span><span style=color:#f29668>*</span><span>object</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ff7733>struct</span><span> kmem_cache_cpu </span><span style=color:#f29668>*</span><span>c</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ff7733>struct</span><span> page </span><span style=color:#f29668>*</span><span>page</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ff7733>unsigned long</span><span> tid</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ff7733>struct</span><span> obj_cgroup </span><span style=color:#f29668>*</span><span>objcg </span><span style=color:#f29668>= </span><span style=color:#f29718>NULL</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	s </span><span style=color:#f29668>= </span><span style=color:#ffb454>slab_pre_alloc_hook</span><span>(s</span><span style=color:#bfbab0cc>, </span><span style=color:#f29668>&</span><span>objcg</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>1</span><span style=color:#bfbab0cc>,</span><span> gfpflags)</span><span style=color:#bfbab0cc>;
</span><span>    </span><span style=color:#ff7733>if</span><span>(</span><span style=color:#f29668>!</span><span>s)
</span><span>        </span><span style=color:#ff7733>return </span><span style=color:#f29718>NULL</span><span style=color:#bfbab0cc>;
</span><span>    </span><span style=color:#f29668>...
</span><span>    </span><span style=color:#f29668>...
</span></code></pre><p>Let us dive into the <strong>slab_pre_alloc_hook</strong>.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>static inline struct</span><span> kmem_cache </span><span style=color:#f29668>*</span><span style=color:#ffb454>slab_pre_alloc_hook</span><span>(</span><span style=color:#ff7733>struct</span><span> kmem_cache </span><span style=color:#f29668>*</span><span style=color:#f29718>s</span><span style=color:#bfbab0cc>,
</span><span>						     gfp_t </span><span style=color:#f29718>flags</span><span>)
</span><span>{
</span><span>	flags </span><span style=color:#f29668>&=</span><span> gfp_allowed_mask</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ffb454>lockdep_trace_alloc</span><span>(flags)</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ffb454>might_sleep_if</span><span>(</span><span style=color:#ffb454>gfpflags_allow_blocking</span><span>(flags))</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>should_failslab</span><span>(s</span><span style=color:#f29668>-></span><span>object_size</span><span style=color:#bfbab0cc>,</span><span> flags</span><span style=color:#bfbab0cc>,</span><span> s</span><span style=color:#f29668>-></span><span>flags))
</span><span>		</span><span style=color:#ff7733>return </span><span style=color:#f29718>NULL</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	</span><span style=color:#ff7733>return </span><span style=color:#ffb454>memcg_kmem_get_cache</span><span>(s</span><span style=color:#bfbab0cc>,</span><span> flags)</span><span style=color:#bfbab0cc>;
</span><span>}
</span></code></pre><p>Masking of bits is done to ensure that the relevant operation is dont. The more interesting part is the <strong>might_sleep_if</strong> function which actually returns a boolean value to decide whether the given flag can cause a sleep or not.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>static inline bool </span><span style=color:#ffb454>gfpflags_allow_blocking</span><span>(</span><span style=color:#ff7733>const</span><span> gfp_t </span><span style=color:#f29718>gfp_flags</span><span>)
</span><span>{
</span><span>	</span><span style=color:#ff7733>return </span><span>(</span><span style=color:#ff7733>bool</span><span> __force)(gfp_flags </span><span style=color:#f29668>&</span><span> __GFP_DIRECT_RECLAIM)</span><span style=color:#bfbab0cc>;
</span><span>}
</span></code></pre><p>The <strong>GFP_KERNEL</strong> flag is actually flag obtained by or'ing 3 lower level flags.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>#define </span><span style=color:#59c2ff>__GFP_DIRECT_RECLAIM	</span><span>((__force gfp_t)___GFP_DIRECT_RECLAIM) </span><span style=font-style:italic;color:#5c6773>/* Caller can reclaim */
</span><span style=color:#ff7733>#define </span><span style=color:#59c2ff>__GFP_RECLAIM </span><span>((__force gfp_t)(___GFP_DIRECT_RECLAIM</span><span style=color:#f29668>|</span><span>___GFP_KSWAPD_RECLAIM))
</span><span style=color:#ff7733>#define </span><span style=color:#59c2ff>GFP_KERNEL	</span><span>(__GFP_RECLAIM </span><span style=color:#f29668>|</span><span> __GFP_IO </span><span style=color:#f29668>|</span><span> __GFP_FS)
</span></code></pre><p>This means, our <strong>gfpflags_allow_blocking</strong> will return a true with <strong>GFP_KERNEL</strong>. This makes sense as getting a free slab may require time and if we sleep , we can give our processor to switch to another task in the meanwhile.<p>Ok so back to <strong>slab_alloc_node</strong>, the next section is -<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span>redo</span><span style=color:#f29668>:
</span><span>	</span><span style=font-style:italic;color:#5c6773>/*
</span><span style=font-style:italic;color:#5c6773>	 * Must read kmem_cache cpu data via this cpu ptr. Preemption is
</span><span style=font-style:italic;color:#5c6773>	 * enabled. We may switch back and forth between cpus while
</span><span style=font-style:italic;color:#5c6773>	 * reading from one cpu area. That does not matter as long
</span><span style=font-style:italic;color:#5c6773>	 * as we end up on the original cpu again when doing the cmpxchg.
</span><span style=font-style:italic;color:#5c6773>	 *
</span><span style=font-style:italic;color:#5c6773>	 * We should guarantee that tid and kmem_cache are retrieved on
</span><span style=font-style:italic;color:#5c6773>	 * the same cpu. It could be different if CONFIG_PREEMPT so we need
</span><span style=font-style:italic;color:#5c6773>	 * to check if it is matched or not.
</span><span style=font-style:italic;color:#5c6773>	 */
</span><span>	</span><span style=color:#ff7733>do </span><span>{
</span><span>		tid </span><span style=color:#f29668>= </span><span style=color:#ffb454>this_cpu_read</span><span>(s</span><span style=color:#f29668>-></span><span>cpu_slab</span><span style=color:#f29668>-></span><span>tid)</span><span style=color:#bfbab0cc>;
</span><span>		c </span><span style=color:#f29668>= </span><span style=color:#ffb454>raw_cpu_ptr</span><span>(s</span><span style=color:#f29668>-></span><span>cpu_slab)</span><span style=color:#bfbab0cc>;
</span><span>	} </span><span style=color:#ff7733>while </span><span>(</span><span style=color:#ffb454>IS_ENABLED</span><span>(CONFIG_PREEMPT) </span><span style=color:#f29668>&&
</span><span>		 </span><span style=color:#ffb454>unlikely</span><span>(tid </span><span style=color:#f29668>!= </span><span style=color:#ffb454>READ_ONCE</span><span>(c</span><span style=color:#f29668>-></span><span>tid)))</span><span style=color:#bfbab0cc>;
</span><span>
</span></code></pre><p><strong>tid</strong> is a unique cpu transaction id.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>#define </span><span style=color:#59c2ff>TID_STEP  </span><span style=color:#ffb454>roundup_pow_of_two</span><span>(CONFIG_NR_CPUS)
</span><span>
</span></code></pre><p>Each CPU has tid initialised to the CPU number and is incremented by the <strong>CONFIG_NR_CPUS</strong> and thus are kept unique.<ul><li>The next while loop is simply for checking if another thread of CPU is trying to call <strong>slab_alloc_node</strong> and if so , its tid will be different and hence the tid is re-read.</ul><pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span>	</span><span style=color:#ffb454>barrier</span><span>()</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>	</span><span style=font-style:italic;color:#5c6773>/*
</span><span style=font-style:italic;color:#5c6773>	 * The transaction ids are globally unique per cpu and per operation on
</span><span style=font-style:italic;color:#5c6773>	 * a per cpu queue. Thus they can be guarantee that the cmpxchg_double
</span><span style=font-style:italic;color:#5c6773>	 * occurs on the right processor and that there was no operation on the
</span><span style=font-style:italic;color:#5c6773>	 * linked list in between.
</span><span style=font-style:italic;color:#5c6773>	 */
</span><span>
</span><span>	object </span><span style=color:#f29668>=</span><span> c</span><span style=color:#f29668>-></span><span>freelist</span><span style=color:#bfbab0cc>;
</span><span>	page </span><span style=color:#f29668>=</span><span> c</span><span style=color:#f29668>-></span><span>page</span><span style=color:#bfbab0cc>;
</span><span>	</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>unlikely</span><span>(</span><span style=color:#f29668>!</span><span>object </span><span style=color:#f29668>|| !</span><span>page </span><span style=color:#f29668>|| !</span><span style=color:#ffb454>node_match</span><span>(page</span><span style=color:#bfbab0cc>,</span><span> node))) {
</span><span>		object </span><span style=color:#f29668>= </span><span style=color:#ffb454>__slab_alloc</span><span>(s</span><span style=color:#bfbab0cc>,</span><span> gfpflags</span><span style=color:#bfbab0cc>,</span><span> node</span><span style=color:#bfbab0cc>,</span><span> addr</span><span style=color:#bfbab0cc>,</span><span> c)</span><span style=color:#bfbab0cc>;
</span><span>	} </span><span style=color:#ff7733>else </span><span>{
</span><span>		</span><span style=color:#ff7733>void </span><span style=color:#f29668>*</span><span>next_object </span><span style=color:#f29668>= </span><span style=color:#ffb454>get_freepointer_safe</span><span>(s</span><span style=color:#bfbab0cc>,</span><span> object)</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>		</span><span style=font-style:italic;color:#5c6773>/*
</span><span style=font-style:italic;color:#5c6773>		 * The cmpxchg will only match if there was no additional
</span><span style=font-style:italic;color:#5c6773>		 * operation and if we are on the right processor.
</span><span style=font-style:italic;color:#5c6773>		 *
</span><span style=font-style:italic;color:#5c6773>		 * The cmpxchg does the following atomically (without lock
</span><span style=font-style:italic;color:#5c6773>		 * semantics!)
</span><span style=font-style:italic;color:#5c6773>		 * 1. Relocate first pointer to the current per cpu area.
</span><span style=font-style:italic;color:#5c6773>		 * 2. Verify that tid and freelist have not been changed
</span><span style=font-style:italic;color:#5c6773>		 * 3. If they were not changed replace tid and freelist
</span><span style=font-style:italic;color:#5c6773>		 *
</span><span style=font-style:italic;color:#5c6773>		 * Since this is without lock semantics the protection is only
</span><span style=font-style:italic;color:#5c6773>		 * against code executing on this cpu *not* from access by
</span><span style=font-style:italic;color:#5c6773>		 * other cpus.
</span><span style=font-style:italic;color:#5c6773>		 */
</span><span>		</span><span style=color:#ff7733>if </span><span>(</span><span style=color:#ffb454>unlikely</span><span>(</span><span style=color:#f29668>!</span><span style=color:#ffb454>this_cpu_cmpxchg_double</span><span>(
</span><span>				s</span><span style=color:#f29668>-></span><span>cpu_slab</span><span style=color:#f29668>-></span><span>freelist</span><span style=color:#bfbab0cc>,</span><span> s</span><span style=color:#f29668>-></span><span>cpu_slab</span><span style=color:#f29668>-></span><span>tid</span><span style=color:#bfbab0cc>,
</span><span>				object</span><span style=color:#bfbab0cc>,</span><span> tid</span><span style=color:#bfbab0cc>,
</span><span>				next_object</span><span style=color:#bfbab0cc>, </span><span style=color:#ffb454>next_tid</span><span>(tid)))) {
</span><span>
</span><span>			</span><span style=color:#ffb454>note_cmpxchg_failure</span><span>(</span><span style=color:#c2d94c>"slab_alloc"</span><span style=color:#bfbab0cc>,</span><span> s</span><span style=color:#bfbab0cc>,</span><span> tid)</span><span style=color:#bfbab0cc>;
</span><span>			</span><span style=color:#ff7733>goto</span><span> redo</span><span style=color:#bfbab0cc>;
</span><span>		}
</span><span>		</span><span style=color:#ffb454>prefetch_freepointer</span><span>(s</span><span style=color:#bfbab0cc>,</span><span> next_object)</span><span style=color:#bfbab0cc>;
</span><span>		</span><span style=color:#ffb454>stat</span><span>(s</span><span style=color:#bfbab0cc>,</span><span> ALLOC_FASTPATH)</span><span style=color:#bfbab0cc>;
</span><span>	}
</span></code></pre><p>The <strong>barrier</strong> simply ensures that read occurs in order.<ul><li><p>The first free object is read into the <strong>object</strong> variable and if the page has no objects left , a call to slab alloc is done to do an entire new allocation from here.</p><li><p>If we have free objects , a call to <strong>get_freepointer_safe()</strong> is made to get the free object.</p><li><p>Following this , a call to <strong>cmpxchg</strong> is made which is to check if the freelist pointer and the tid have not been changed , and if not , they are respectively updated with their new values of <strong>next_object</strong> and <strong>next_tid</strong>.</p><li><p>The <strong>cmpxchg</strong> happens atomically and hence there is no need of locking here.</p><li><p>Moving on , we have a call to <strong>prefetch_freepointer</strong> which just adds our object with the offset and basically sets up the next free chunk in the list to the cache line.</p><li><p>Finally , <strong>slab_post_alloc_hook</strong> is called which returns the modified slab to the memory control group.</p></ul><p>In short , the entire process of allocating memory using slab allocator is -<ul><li><p>Suppose kernel is asked for a size of x.</p><li><p>Slab allocator looks in the slab index for the slab that holds object of size x.</p><li><p>The slab allocator gets a pointer to slab where objects are stored.</p><li><p>Finds the first empty slot</p><li><p>Similar process is used for freeing the allocated slab.</p></ul><p>So why'd we move from slab allocator , and now linux uses the SLUB allocator?<p>You see , slab had its own scalability problems. Slab object queues exist per node per CPU. For very large systems, such queues may grow exponentially and hence at some point of processing , may consume the entire system memory which is not what we need.<p>Hence , the need of a more scalable allocator was the need of the hour.<h2 id=the-slub-allocator>The SLUB Allocator</h2><p>It keeps the same inner principles of the SLAB allocator but drops the requirements of the complex queues and per slab metadata. Information about all the active slabs is kept in the <strong>kmem_cache</strong> structure. Per-slab metadata is kept in three basic fields<pre style=background:#0f1419;color:#bfbab0><code><span>void *freelist;
</span><span>short unsigned int inuse;
</span><span>short unsigned int offset;
</span><span>
</span></code></pre><ul><li><p>Freelist is the pointer to the first free chunk in the slab.</p><li><p>inuse is the count of number of objects being used.</p><li><p>offset is offset to the next free chunk.</p></ul><p>SLUB also introduces the coalescing of slabs which greatly reduces the memory overhead.<h2 id=conclusion>Conclusion</h2><p>The implementation of kmalloc is a very interesting process of linux kernel memory management. The techniques employed to implement kmalloc are quite fascinating and thought provoking.<p>Here are the references to everything covered in this post :<ul><li><a href=https://elixir.bootlin.com/linux/latest/source/mm/slab.c#L3664>Source Code</a><li><a href=https://ruffell.nz/programming/writeups/2019/02/15/looking-at-kmalloc-and-the-slub-memory-allocator.html>An awesome post on kmalloc internals</a><li><a href=https://www.kernel.org/doc/html/latest/vm/numa.html>A little on NUMA</a></ul></section></article></main></div>